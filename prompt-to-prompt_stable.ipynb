{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Prompt-to-Prompt with Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils\n",
    "import seq_aligner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update ```MY_TOKEN``` with your token.\n",
    "Set ```LOW_RESOURCE``` to ```True``` for running on 12GB GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MY_TOKEN = '<replace with your token>'\n",
    "LOW_RESOURCE = True\n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    torch_dtype=torch.float16\n",
    "    #     use_auth_token=MY_TOKEN\n",
    ").to(device)\n",
    "\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_vae_slicing()\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "# pipe.enable_vae_tiling()\n",
    "\n",
    "tokenizer = pipe.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Prompt-to-Prompt Attention Controllers\n",
    "Our main logic is implemented in the `forward` call in an `AttentionControl` object.\n",
    "The forward is called in each attention layer of the diffusion model and it can modify the input attention weights `attn`.\n",
    "\n",
    "`is_cross`, `place_in_unet in (\"down\", \"mid\", \"up\")`, `AttentionControl.cur_step` help us track the exact attention layer and timestamp during the diffusion inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBlend:\n",
    "    def __call__(self, x_t, attention_store):\n",
    "        k = 1\n",
    "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "        maps = [\n",
    "            item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS)\n",
    "            for item in maps\n",
    "        ]\n",
    "        maps = torch.cat(maps, dim=1)\n",
    "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
    "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 + 1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.threshold)\n",
    "        #         mask = (mask[:1] + mask[1:]).float()\n",
    "        mask = (mask[:1] + mask[1:]).half()  # MOD: half\n",
    "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "\n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], threshold=0.3):\n",
    "        alpha_layers = torch.zeros(len(prompts), 1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        alpha_layers = alpha_layers.half()  # MOD: half()\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class AttentionControl(abc.ABC):\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def between_steps(self):\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2 :] = self.forward(attn[h // 2 :], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "\n",
    "\n",
    "class AttentionStore(AttentionControl):\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\n",
    "            \"down_cross\": [],\n",
    "            \"mid_cross\": [],\n",
    "            \"up_cross\": [],\n",
    "            \"down_self\": [],\n",
    "            \"mid_self\": [],\n",
    "            \"up_self\": [],\n",
    "        }\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32**2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {\n",
    "            key: [item / self.cur_step for item in self.attention_store[key]]\n",
    "            for key in self.attention_store\n",
    "        }\n",
    "        return average_attention\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "\n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "\n",
    "    def replace_self_attention(self, attn_base, att_replace):\n",
    "        if att_replace.shape[2] <= 16**2:\n",
    "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "        else:\n",
    "            return att_replace\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        # print(attn.shape, f'is_cross: {is_cross}')\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (\n",
    "            self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]\n",
    "        ):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                # print('before replace_cross_attention', attn.shape)\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = (\n",
    "                    alpha_words * self.replace_cross_attention(attn_base, attn_repalce)\n",
    "                    + (1 - alpha_words) * attn_repalce\n",
    "                )\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                # print('before replace_self_attention')\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: Union[\n",
    "            float, Tuple[float, float], Dict[str, Tuple[float, float]]\n",
    "        ],\n",
    "        self_replace_steps: Union[float, Tuple[float, float]],\n",
    "        local_blend: Optional[LocalBlend],\n",
    "    ):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(\n",
    "            prompts, num_steps, cross_replace_steps, tokenizer\n",
    "        ).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = [\n",
    "            int(num_steps * self_replace_steps[0]),\n",
    "            int(num_steps * self_replace_steps[1]),\n",
    "        ]\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        #         print(attn_base.shape, self.mapper.shape)\n",
    "        return torch.einsum(\"...hpw,bwn->bhpn\", attn_base, self.mapper)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "    ):\n",
    "        super(AttentionReplace, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "        self.mapper = self.mapper.half()  # MOD: half()\n",
    "\n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        # attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_base_replace = attn_base[0, :, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "    ):\n",
    "        super(AttentionRefine, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(\n",
    "                attn_base, att_replace\n",
    "            )\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        equalizer,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "        controller: Optional[AttentionControlEdit] = None,\n",
    "    ):\n",
    "        super(AttentionReweight, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(\n",
    "    text: str,\n",
    "    word_select: Union[int, Tuple[int, ...]],\n",
    "    values: Union[List[float], Tuple[float, ...]],\n",
    "):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(len(values), 77)\n",
    "    values = torch.tensor(values, dtype=torch.float32)\n",
    "    for word in word_select:\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = values\n",
    "    return equalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttentionControl Inheritance Tree\n",
    "\n",
    "- AttentionControl\n",
    "    - EmptyControl / AttentionStore\n",
    "\n",
    "- AttentionStore\n",
    "    - AttentionControlEdit: replace_cross_attention(), replace_self_attention() only touches the bottle neck layer??\n",
    "        - AttentionReplace / AttentionRefine / AttentionReweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def aggregate_attention(\n",
    "    attention_store: AttentionStore,\n",
    "    res: int,\n",
    "    from_where: List[str],\n",
    "    is_cross: bool,\n",
    "    select: int,\n",
    "):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res**2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            #             print('location', location, item.shape, num_pixels)\n",
    "            if (\n",
    "                is_cross and item.shape[2] == num_pixels\n",
    "            ):  ## only aggregate those with the supplied resolution (e.g., 16)\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[\n",
    "                    select\n",
    "                ]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def show_cross_attention(\n",
    "    attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0\n",
    "):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "\n",
    "\n",
    "def show_self_attention_comp(\n",
    "    attention_store: AttentionStore,\n",
    "    res: int,\n",
    "    from_where: List[str],\n",
    "    max_com=10,\n",
    "    select: int = 0,\n",
    "):\n",
    "    attention_maps = (\n",
    "        aggregate_attention(attention_store, res, from_where, False, select)\n",
    "        .numpy()\n",
    "        .reshape((res**2, res**2))\n",
    "    )\n",
    "    u, s, vh = np.linalg.svd(\n",
    "        attention_maps - np.mean(attention_maps, axis=1, keepdims=True)\n",
    "    )\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_and_display(\n",
    "    prompts, controller, latent=None, run_baseline=False, generator=None\n",
    "):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(\n",
    "            prompts,\n",
    "            EmptyControl(),\n",
    "            latent=latent,\n",
    "            run_baseline=False,\n",
    "            generator=generator,\n",
    "        )\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = ptp_utils.text2image_ldm_stable(\n",
    "        pipe,\n",
    "        prompts,\n",
    "        controller,\n",
    "        latent=latent,\n",
    "        num_inference_steps=NUM_DIFFUSION_STEPS,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        generator=generator,\n",
    "        low_resource=LOW_RESOURCE,\n",
    "    )\n",
    "    ptp_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Cross-Attention Visualization\n",
    "First let's generate an image and visualize the cross-attention maps for each word in the prompt.\n",
    "Notice, we normalize each map to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(controller.attention_store['down_cross'])\n",
    "# controller.attention_store['down_cross'][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prompts = [\"A painting of a squirrel eating a burger\"]\n",
    "# g_cpu = torch.Generator().manual_seed(8888)\n",
    "\n",
    "prompts = [\"A photo of a squirrel eating a burger\"]\n",
    "g_cpu = torch.Generator().manual_seed(2)\n",
    "\n",
    "controller = AttentionStore()\n",
    "image, x_t = run_and_display(\n",
    "    prompts, controller, latent=None, run_baseline=False, generator=g_cpu\n",
    ")\n",
    "show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))\n",
    "del controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Replacement edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"A photo of a squirrel eating a burger\",\n",
    "#     \"A photo of a lion eating a burger\",\n",
    "# ]\n",
    "\n",
    "# controller = AttentionReplace(\n",
    "#     prompts, NUM_DIFFUSION_STEPS,\n",
    "#     cross_replace_steps=0.8,\n",
    "#     self_replace_steps=0.4,\n",
    "# )\n",
    "\n",
    "# images, x_t = ptp_utils.text2image_ldm_stable(\n",
    "#     pipe,\n",
    "#     prompts,\n",
    "#     controller,\n",
    "#     latent=None,\n",
    "#     num_inference_steps=NUM_DIFFUSION_STEPS,\n",
    "#     guidance_scale=GUIDANCE_SCALE,\n",
    "#     generator=torch.Generator().manual_seed(2),\n",
    "#     low_resource=LOW_RESOURCE\n",
    "# )\n",
    "\n",
    "# ptp_utils.view_images(images)\n",
    "# del controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i, image in enumerate(images):\n",
    "# #     image_pil = Image.fromarray(image)\n",
    "# #     image_pil.save(f'{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "# prompts = [\n",
    "#     \"A photo of a squirrel eating a burger\",\n",
    "#     \"A photo of a lion eating a burger\",\n",
    "# #     \"A photo of cat sit next dog a\",\n",
    "# #     \"cat dog a b c d e f\",\n",
    "# ]\n",
    "\n",
    "# # run through different cross_replace_steps (cr) and self_replace_steps (sr)\n",
    "# # 1.0 = always replace\n",
    "# # 0.0 = never replace\n",
    "\n",
    "# for i, [cr, sr, title] in enumerate([\n",
    "#     (0.0, 0.0, 'Run two prompts separately w/o word swap (base case)'),\n",
    "#     (0.8, 0.4, 'prompt-to-prompt default parameter'),\n",
    "#     (1.0, 0.0, 'prompt-to-prompt only swap cross attention'),\n",
    "#     (0.0, 1.0, 'prompt-to-prompt only swap self attention'),\n",
    "#     (0.2, 0.2, 'prompt-to-prompt swap 0.2 of self and cross'),\n",
    "#     (0.5, 0.5, 'prompt-to-prompt swap 0.5 of self and cross'),\n",
    "#     (0.7, 0.7, 'prompt-to-prompt swap 0.7 of self and cross'),\n",
    "#     (0.9, 0.9, 'prompt-to-prompt swap 0.9 of self and cross'),\n",
    "# ]):\n",
    "#     controller = AttentionReplace(\n",
    "#         prompts, NUM_DIFFUSION_STEPS,\n",
    "#         cross_replace_steps=cr,\n",
    "#         self_replace_steps=sr,\n",
    "#     )\n",
    "#     images, _ = ptp_utils.text2image_ldm_stable(\n",
    "#         pipe,\n",
    "#         prompts,\n",
    "#         controller,\n",
    "#         latent=x_t,\n",
    "#         num_inference_steps=NUM_DIFFUSION_STEPS,\n",
    "#         guidance_scale=GUIDANCE_SCALE,\n",
    "# #         generator=torch.Generator().manual_seed(2),\n",
    "#         low_resource=LOW_RESOURCE,\n",
    "#     )\n",
    "\n",
    "#     img_grid = make_grid(torch.from_numpy(images).permute(0,3,1,2)).permute(1,2,0).numpy()\n",
    "\n",
    "#     plt.imshow(img_grid)\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "#     del controller, images, _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Modify Cross-Attention injection #steps for specific words\n",
    "Next, we can reduce the restriction on our lion by reducing the number of cross-attention injection with respect to the word \"lion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"A painting of a squirrel eating a burger\",\n",
    "#     \"A painting of a lion eating a burger\"\n",
    "# ]\n",
    "# controller = AttentionReplace(\n",
    "#     prompts,\n",
    "#     NUM_DIFFUSION_STEPS,\n",
    "#     cross_replace_steps={\"default_\": 1., \"lion\": .4},\n",
    "#     self_replace_steps=0.4\n",
    "# )\n",
    "# _ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)\n",
    "# del controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Local Edit\n",
    "Lastly, if we want to preseve the original burger, we can apply a local edit with respect to the squirrel and the lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A painting of a squirrel eating a burger\",\n",
    "    \"A painting of a lion eating a burger\",\n",
    "]\n",
    "lb = LocalBlend(prompts, (\"squirrel\", \"lion\"))\n",
    "controller = AttentionReplace(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps={\"default_\": 1.0, \"lion\": 0.4},\n",
    "    self_replace_steps=0.4,\n",
    "    local_blend=lb,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(controller.mapper[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A painting of a squirrel eating a burger\",\n",
    "    \"A painting of a squirrel eating a lasagne\",\n",
    "]\n",
    "lb = LocalBlend(prompts, (\"burger\", \"lasagne\"))\n",
    "controller = AttentionReplace(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps={\"default_\": 1.0, \"lasagne\": 0.2},\n",
    "    self_replace_steps=0.4,\n",
    "    local_blend=lb,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=True)\n",
    "del controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Refinement edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A              painting of a squirrel eating a burger\",\n",
    "    \"A neoclassical painting of a squirrel eating a burger\",\n",
    "]\n",
    "\n",
    "controller = AttentionRefine(\n",
    "    prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=0.5, self_replace_steps=0.2\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.tokenizer.convert_ids_to_tokens(\n",
    "    pipe.tokenizer(\"A neoclassical painting\").input_ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"a photo of a house on a mountain\",\n",
    "    \"a photo of a house on a mountain at fall\",\n",
    "]\n",
    "\n",
    "\n",
    "controller = AttentionRefine(\n",
    "    prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=0.8, self_replace_steps=0.4\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t)\n",
    "del controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"a photo of a house on a mountain\",\n",
    "    \"a photo of a house on a mountain at winter\",\n",
    "]\n",
    "\n",
    "\n",
    "controller = AttentionRefine(\n",
    "    prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=0.8, self_replace_steps=0.4\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t)\n",
    "del controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"soup\", \"pea soup\"]\n",
    "\n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "\n",
    "controller = AttentionRefine(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    local_blend=lb,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)\n",
    "del controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Re-Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a smiling bunny doll\"] * 2\n",
    "\n",
    "### pay 3 times more attention to the word \"smiling\"\n",
    "equalizer = get_equalizer(prompts[1], (\"smiling\",), (5,))\n",
    "controller = AttentionReweight(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    equalizer=equalizer,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.cross_replace_alpha.squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(controller.cross_replace_alpha.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"pink bear riding a bicycle\"] * 2\n",
    "\n",
    "### we don't wont pink bikes, only pink bear.\n",
    "### we reduce the amount of pink but apply it locally on the bikes (attention re-weight + local mask )\n",
    "\n",
    "### pay less attention to the word \"pink\"\n",
    "equalizer = get_equalizer(prompts[1], (\"pink\",), (-1,))\n",
    "\n",
    "### apply the edit on the bikes\n",
    "lb = LocalBlend(prompts, (\"bicycle\", \"bicycle\"))\n",
    "controller = AttentionReweight(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    equalizer=equalizer,\n",
    "    local_blend=lb,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are my croutons?\n",
    "It might be useful to use Attention Re-Weighting with a previous edit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"soup\", \"pea soup with croutons\"]\n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "controller = AttentionRefine(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    local_blend=lb,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with more attetnion to `\"croutons\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"soup\", \"pea soup with croutons\"]\n",
    "\n",
    "\n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "controller_a = AttentionRefine(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    local_blend=lb,\n",
    ")\n",
    "\n",
    "### pay 3 times more attention to the word \"croutons\"\n",
    "equalizer = get_equalizer(prompts[1], (\"croutons\",), (3,))\n",
    "controller = AttentionReweight(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    equalizer=equalizer,\n",
    "    local_blend=lb,\n",
    "    controller=controller_a,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"potatos\", \"fried potatos\"]\n",
    "lb = LocalBlend(prompts, (\"potatos\", \"potatos\"))\n",
    "controller = AttentionRefine(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    local_blend=lb,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"potatos\", \"fried potatos\"]\n",
    "lb = LocalBlend(prompts, (\"potatos\", \"potatos\"))\n",
    "# controller = AttentionRefine( # typo? Should be controller_a instead of controller?\n",
    "controller_a = AttentionRefine(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    local_blend=lb,\n",
    ")\n",
    "\n",
    "### pay 50 times more attention to the word \"fried\"\n",
    "equalizer = get_equalizer(prompts[1], (\"fried\",), (50,))\n",
    "controller = AttentionReweight(\n",
    "    prompts,\n",
    "    NUM_DIFFUSION_STEPS,\n",
    "    cross_replace_steps=0.8,\n",
    "    self_replace_steps=0.4,\n",
    "    equalizer=equalizer,\n",
    "    local_blend=lb,\n",
    "    controller=controller_a,\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalizer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
